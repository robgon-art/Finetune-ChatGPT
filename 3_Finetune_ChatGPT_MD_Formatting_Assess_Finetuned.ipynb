{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZkK6t3pIqBMtY6CKwR8rjCjinW3Bz-pF",
      "authorship_tag": "ABX9TyOiV44hdL0G6qOfaYjoQPTe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robgon-art/Finetune-ChatGPT/blob/main/3_Finetune_ChatGPT_MD_Formatting_Assess_Finetuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune-ChatGPT\n",
        "## How you can fine-tune OpenAI's GPT-3.5 Turbo model to perform new tasks using your custom data"
      ],
      "metadata": {
        "id": "YCkGb8OETJde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai jiwer tiktoken\n",
        "!gdown 15_92FZ9FgINnxYcvu7TYzx0GPXY9jsVp\n",
        "!gdown 15Vib6A4h82G4X467x_AGypFW-GBJd5gm\n",
        "!unzip robgon_article_parts_txt.zip\n",
        "!unzip robgon_article_parts_md.zip"
      ],
      "metadata": {
        "id": "5-0kSKAqJpwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20a3fc2-ea38-42d3-ede5-e1cad5d6ccb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jiwer\n",
            "  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: rapidfuzz, tiktoken, jiwer, openai\n",
            "Successfully installed jiwer-3.0.3 openai-0.28.0 rapidfuzz-3.2.0 tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "openai.api_key = \"your_api_key\""
      ],
      "metadata": {
        "id": "FyTpep2QkYc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "txt_files = glob.glob('/content/robgon_article_parts_txt/*.txt')\n",
        "print(len(txt_files))\n",
        "txt_files.sort()\n",
        "for h in txt_files[-12:]:\n",
        "  print(h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GMh_03cBSL0",
        "outputId": "149fc335-20f5-4282-93d2-164635a8d2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "144\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_txt/2023-05-10_writing-songs-with-gpt-4-part-2-chords_1.txt\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_txt/2023-05-10_writing-songs-with-gpt-4-part-2-chords_2.txt\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_txt/2023-05-10_writing-songs-with-gpt-4-part-2-chords_3.txt\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_txt/2023-05-10_writing-songs-with-gpt-4-part-2-chords_4.txt\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_txt/2023-06-14_writing-songs-with-gpt-4-part-3-melodies_1.txt\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_txt/2023-06-14_writing-songs-with-gpt-4-part-3-melodies_2.txt\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_txt/2023-06-14_writing-songs-with-gpt-4-part-3-melodies_3.txt\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_txt/2023-06-14_writing-songs-with-gpt-4-part-3-melodies_4.txt\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_txt/2023-07-25_muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai_1.txt\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_txt/2023-07-25_muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai_2.txt\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_txt/2023-07-25_muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai_3.txt\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_txt/2023-07-25_muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai_4.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "md_files = glob.glob('/content/robgon_article_parts_md/*.md')\n",
        "print(len(md_files))\n",
        "md_files.sort()\n",
        "for h in md_files[-12:]:\n",
        "  print(h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_8ZoUgSDlp4",
        "outputId": "9127b0c0-c4c0-481d-b0ad-9440bd387e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "144\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_md/2023-05-10_writing-songs-with-gpt-4-part-2-chords_1.md\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_md/2023-05-10_writing-songs-with-gpt-4-part-2-chords_2.md\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_md/2023-05-10_writing-songs-with-gpt-4-part-2-chords_3.md\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_md/2023-05-10_writing-songs-with-gpt-4-part-2-chords_4.md\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_md/2023-06-14_writing-songs-with-gpt-4-part-3-melodies_1.md\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_md/2023-06-14_writing-songs-with-gpt-4-part-3-melodies_2.md\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_md/2023-06-14_writing-songs-with-gpt-4-part-3-melodies_3.md\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_md/2023-06-14_writing-songs-with-gpt-4-part-3-melodies_4.md\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_md/2023-07-25_muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai_1.md\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_md/2023-07-25_muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai_2.md\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_md/2023-07-25_muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai_3.md\n",
            "/content/drive/MyDrive/Finetune_ChatGPT/robgon_article_parts_md/2023-07-25_muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai_4.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(txt_files[-4], 'r') as f:\n",
        "  content_string = f.read()"
      ],
      "metadata": {
        "id": "okv3tcwOEqWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(content_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HooLvtGiE3ZI",
        "outputId": "cfa87ae4-7ec7-4f07-cdda-5daf7ebff7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Muybridge Derby: Bringing Animal Locomotion Photographs to Life with AI\n",
            "How I used Midjourney and RunwayML to transform Eadweard Muybridge’s photo sequences into high-resolution videos\n",
            "Robert A. Gonsalves\n",
            "Towards Data Science\n",
            "Jul 25, 2023\n",
            "Muybridge Derby - https://medium.com/towards-data-science/muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai-b1918e6622ec\n",
            "Background\n",
            "I’m sure you’ve seen the series of images of a galloping horse by 19th-century English photographer Eadweard Muybridge. As a refresher, here is a GIF animation that shows one of his more famous photo series.\n",
            "And here’s a portrait of Muybridge with an illustration of the apparatus he built to capture the photo series.\n",
            "Eadweard Muybridge\n",
            "Muybridge was a nature photographer commissioned by the Governor of California, Leland Stanford, to document his mansion and possessions. Stanford posed an exciting challenge to Muybridge: could he take clear pictures of a galloping horse?\n",
            "1872 was the year that Muybridge began his zealous involvement with motion photography. He was commissioned by Governor Leland Stanford to photograph the moving gait of his racehorse, Occident. Until this time the gait of a moving horse had been a mystery. When did the feet touch the ground? Did all four feet ever leave the ground at the same time? Painting the feet of the galloping horse had been an unsolved problem for artists. ... [He used] 12 cameras, each hooked to an electrical apparatus that would trip the shutters as the horse galloped past. … Muybridge invented the zoopraxiscope in 1879, a machine that allowed him to project up to two hundred single images on a screen. In 1880 he gave his first presentation of projected moving pictures on a screen to a group at the California School of Fine Arts, thus becoming the father of motion pictures.- Vi Whitmire [1]\n",
            "And Muybridge didn’t just take pictures of moving horses. He created similar sequences of moving cats, dogs, buffaloes, ostriches, people, etc.\n",
            "Muybridge Derby\n",
            "For this project, I wanted to see if I could use AI systems to transform Muybridge’s animal locomotion photographs into high-resolution, full-color videos. After experimenting with various techniques, I changed the original sequences to be more realistic using a combination of  Midjourney  to create reference frames from text prompts and RunwayML’s Gen-1 Video Generator. For fun, I made a short animation, “Muybridge Derby,” showcasing the work. Here it is.\n",
            "In the following sections, I will describe how I transformed the locomotion sequences, generated the background scroll, and combined the elements to create the animation.\n",
            "Using Midjourney to Generate Reference Frames\n",
            "As a prerequisite for transforming a Muybridge photo series into a high-def video, I generated a high-resolution reference frame using one of the original series’s photos and a text prompt in Midjourney.\n",
            "For example, here is the prompt I used for generating the reference frame of the horse and jockey, “ a man wearing a blue cap, blue jacket, white pants, and black boots riding a brown horse with a white background - -  ar 4:3 .”   Note that the --ar 4:3 parameter indicates the aspect ratio of 4:3.\n",
            "I pasted in a link to the image of Muybridge’s frame number 2 along with the prompt into Midjourney, and it produced four thumbnail images. All four generated images were pretty good. I liked the details and texture of the images, including the look of the jockey’s clothes and the shininess of the horse’s coat. None of them matched the original pose of the horse exactly, but I found out that it doesn’t matter when stylizing a video. The video stylizer in RunwayML only picks up on the general look of the image. I chose the thumbnail image at the lower right (outlined in green) and made some edits in Photoshop; I flipped the image horizontally, changed the hue of the horse to brown, and changed the style of the jockey’s cap.\n",
            "I repeated this process for the other four animals in the animation, a cat, a buffalo, an elephant, and an ostrich. Here are the results. You can see an image from one of Muybridge’s photo series in the left column below. The middle column shows the results from Midjourney using the Muybridge image and the text, like “a full-color photo of a cat running on a dirt track, side view, -- ar 4:3.” The selected thumbnail is outlined in green. The right column shows the selected image, cleaned up a bit and flipped horizontally if needed, in Photoshop.\n",
            "The Midjourney system did a great job generating the reference images. The details of the animals are amazing. You can click on any of the images to zoom in and see. Again, it didn’t precisely match the pose in the reference image, but the overall quality of the renderings was excellent. For more information on Midjourney, you can check out my earlier article  here .\n",
            "Next, I will show you how I used reference images to transform the photo series with RunwayML.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"You render plain text into markdown format.\""
      ],
      "metadata": {
        "id": "ve_zfRe0F0wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = openai.ChatCompletion.create(\n",
        "  model=\"ft:gpt-3.5-turbo-0613:personal:xxx:yyy\", # your model name\n",
        "  temperature = 0.2,\n",
        "  top_p = 0.1,\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": content_string},\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "vO9e_Kp9EFva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HbTZGZhEY75",
        "outputId": "4c7d8e0a-ae67-414d-feac-f9187caad6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Muybridge Derby: Bringing Animal Locomotion Photographs to Life with AI\n",
            "## How I used Midjourney and RunwayML to transform Eadweard Muybridge’s photo sequences into high-resolution videos\n",
            "Robert A. Gonsalves\n",
            "</br>Jul 25, 2023</br>\n",
            "Muybridge Derby - https://medium.com/towards-data-science/muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai-b1918e6622ec</br></br>\n",
            "# Background\n",
            "I’m sure you’ve seen the series of images of a galloping horse by 19th-century English photographer Eadweard Muybridge. As a refresher, here is a GIF animation that shows one of his more famous photo series.\n",
            "And here’s a portrait of Muybridge with an illustration of the apparatus he built to capture the photo series.\n",
            "# Eadweard Muybridge\n",
            "Muybridge was a nature photographer commissioned by the Governor of California, Leland Stanford, to document his mansion and possessions. Stanford posed an exciting challenge to Muybridge: could he take clear pictures of a galloping horse?\n",
            "1872 was the year that Muybridge began his zealous involvement with motion photography. He was commissioned by Governor Leland Stanford to photograph the moving gait of his racehorse, Occident. Until this time the gait of a moving horse had been a mystery. When did the feet touch the ground? Did all four feet ever leave the ground at the same time? Painting the feet of the galloping horse had been an unsolved problem for artists. ... [He used] 12 cameras, each hooked to an electrical apparatus that would trip the shutters as the horse galloped past. … Muybridge invented the zoopraxiscope in 1879, a machine that allowed him to project up to two hundred single images on a screen. In 1880 he gave his first presentation of projected moving pictures on a screen to a group at the California School of Fine Arts, thus becoming the father of motion pictures.- Vi Whitmire [1]\n",
            "And Muybridge didn’t just take pictures of moving horses. He created similar sequences of moving cats, dogs, buffaloes, ostriches, people, etc.\n",
            "# Muybridge Derby\n",
            "For this project, I wanted to see if I could use AI systems to transform Muybridge’s animal locomotion photographs into high-resolution, full-color videos. After experimenting with various techniques, I changed the original sequences to be more realistic using a combination of  Midjourney  to create reference frames from text prompts and RunwayML’s Gen-1 Video Generator. For fun, I made a short animation, “Muybridge Derby,” showcasing the work. Here it is.\n",
            "In the following sections, I will describe how I transformed the locomotion sequences, generated the background scroll, and combined the elements to create the animation.\n",
            "# Using Midjourney to Generate Reference Frames\n",
            "As a prerequisite for transforming a Muybridge photo series into a high-def video, I generated a high-resolution reference frame using one of the original series’s photos and a text prompt in Midjourney.\n",
            "For example, here is the prompt I used for generating the reference frame of the horse and jockey, “ a man wearing a blue cap, blue jacket, white pants, and black boots riding a brown horse with a white background - -  ar 4:3 .”   Note that the --ar 4:3 parameter indicates the aspect ratio of 4:3.\n",
            "I pasted in a link to the image of Muybridge’s frame number 2 along with the prompt into Midjourney, and it produced four thumbnail images. All four generated images were pretty good. I liked the details and texture of the images, including the look of the jockey’s clothes and the shininess of the horse’s coat. None of them matched the original pose of the horse exactly, but I found out that it doesn’t matter when stylizing a video. The video stylizer in RunwayML only picks up on the general look of the image. I chose the thumbnail image at the lower right (outlined in green) and made some edits in Photoshop; I flipped the image horizontally, changed the hue of the horse to brown, and changed the style of the jockey’s cap.\n",
            "I repeated this process for the other four animals in the animation, a cat, a buffalo, an elephant, and an ostrich. Here are the results. You can see an image from one of Muybridge’s photo series in the left column below. The middle column shows the results from Midjourney using the Muybridge image and the text, like “a full-color photo of a cat running on a dirt track, side view, -- ar 4:3.” The selected thumbnail is outlined in green. The right column shows the selected image, cleaned up a bit and flipped horizontally if needed, in Photoshop.\n",
            "The Midjourney system did a great job generating the reference images. The details of the animals are amazing. You can click on any of the images to zoom in and see. Again, it didn’t precisely match the pose in the reference image, but the overall quality of the renderings was excellent. For more information on Midjourney, you can check out my earlier article  here .\n",
            "Next, I will show you how I used reference images to transform the photo series with RunwayML.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write out the results to an md file\n",
        "\n",
        "with open('hupothesis.md', 'w') as f:\n",
        "  f.write(results[\"choices\"][0][\"message\"][\"content\"])\n"
      ],
      "metadata": {
        "id": "ReJM-gHG_73b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(md_files[-4], 'r') as f:\n",
        "  reference = f.read()\n",
        "with open('reference.md', 'w') as f:\n",
        "  f.write(reference)"
      ],
      "metadata": {
        "id": "DOJRUg1vVDa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jiwer\n",
        "\n",
        "hypothesis = results[\"choices\"][0][\"message\"][\"content\"]\n",
        "error = jiwer.cer(reference, hypothesis)\n",
        "print(round(error,5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YQv5QM6Tfx7",
        "outputId": "730e9d72-d656-4627-e313-cf1ef426d5f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jiwer\n",
        "\n",
        "hypothesis = results[\"choices\"][0][\"message\"][\"content\"]\n",
        "error = jiwer.cer(reference, hypothesis)\n",
        "print(round(error,5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca133739-cd3c-4cd1-d218-a5aaa843e937",
        "id": "8GXjbox_-Gsn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_md_format(input_string):\n",
        "    results = openai.ChatCompletion.create(\n",
        "        model=\"ft:gpt-3.5-turbo-0613:personal:xxx:yyy\", # your model name\n",
        "        temperature = 0.2,\n",
        "        top_p = 0.1,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": prompt},\n",
        "            {\"role\": \"user\", \"content\": input_string},\n",
        "        ]\n",
        "    )\n",
        "    return results[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return \"foo\"\n",
        "\n",
        "cers = []\n",
        "\n",
        "for txt_file, md_file in zip(txt_files[-12:], md_files[-12:]):\n",
        "    # Read the content from the txt file\n",
        "    with open(txt_file, 'r') as f:\n",
        "        content_string = f.read()\n",
        "\n",
        "    # Get the predicted markdown format\n",
        "    predicted_md = predict_md_format(content_string)\n",
        "\n",
        "    # Write out the results to a file with a similar name\n",
        "    results_file_name = md_file.replace('.md', '_predicted.md')\n",
        "    results_file_name = md_file.replace('robgon_article_parts_md', 'robgon_article_parts_predicted_finetuned_md')\n",
        "    with open(results_file_name, 'w') as f:\n",
        "        f.write(predicted_md)\n",
        "\n",
        "    # Read the reference markdown content\n",
        "    with open(md_file, 'r') as f:\n",
        "        reference_md = f.read()\n",
        "\n",
        "    # Compute the CER between the predicted and reference markdown\n",
        "    fname = txt_file.split(\"/\")[-1]\n",
        "    error = jiwer.cer(reference_md, predicted_md)\n",
        "    print(f\"CER for {fname} : {round(error, 5)}\")\n",
        "    cers.append(error)\n",
        "\n",
        "avg_cer= round(sum(cers)/len(cers), 5)\n",
        "print(f\"Average CER: {avg_cer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB6eZt7OXFY-",
        "outputId": "88ffb388-6d55-468a-efcb-9abd36279477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CER for 2023-05-10_writing-songs-with-gpt-4-part-2-chords_1.txt : 0.00767\n",
            "CER for 2023-05-10_writing-songs-with-gpt-4-part-2-chords_2.txt : 0.03916\n",
            "CER for 2023-05-10_writing-songs-with-gpt-4-part-2-chords_3.txt : 0.01741\n",
            "CER for 2023-05-10_writing-songs-with-gpt-4-part-2-chords_4.txt : 0.0\n",
            "CER for 2023-06-14_writing-songs-with-gpt-4-part-3-melodies_1.txt : 0.003\n",
            "CER for 2023-06-14_writing-songs-with-gpt-4-part-3-melodies_2.txt : 0.20062\n",
            "CER for 2023-06-14_writing-songs-with-gpt-4-part-3-melodies_3.txt : 0.02049\n",
            "CER for 2023-06-14_writing-songs-with-gpt-4-part-3-melodies_4.txt : 0.0\n",
            "CER for 2023-07-25_muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai_1.txt : 0.00101\n",
            "CER for 2023-07-25_muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai_2.txt : 0.0\n",
            "CER for 2023-07-25_muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai_3.txt : 0.00022\n",
            "CER for 2023-07-25_muybridge-derby-bringing-animal-locomotion-photographs-to-life-with-ai_4.txt : 0.00138\n",
            "Average CER: 0.02425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate median of cers\n",
        "cers.sort()\n",
        "median_cer = round(cers[len(cers) // 2], 5)\n",
        "print(f\"Median CER: {median_cer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFnitfOWjg9b",
        "outputId": "63044ed5-0918-4b40-d374-9537a3f44c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Median CER: 0.003\n"
          ]
        }
      ]
    }
  ]
}